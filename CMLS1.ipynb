{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMLS1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/husaintheking/CMLS1/blob/main/CMLS1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4wY7Y9LePpG"
      },
      "source": [
        "### Lib import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96hpSTTxEhFZ"
      },
      "source": [
        "import librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import csv\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "#Keras\n",
        "import keras\n",
        "\n",
        "#https://medium.com/@sdoshi579/classification-of-music-into-different-genres-using-keras-82ab5339efe0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwII1ZcskpTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3600b2d6-2d66-4f82-e04f-20fa4bda9b2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y7VCBsZ5uA-",
        "outputId": "7ae5c155-6790-4c7f-f47a-d2171ea6b622"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Input\"\n",
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10h63gkOCyEstkEYfz8l4GXxjT18LPixT/Input\n",
            "\u001b[0m\u001b[01;34mDistortion\u001b[0m/  \u001b[01;34mNoFX\u001b[0m/  \u001b[01;34mTremolo\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfS1sFyPFhiz"
      },
      "source": [
        "# creare un array con i nomi delle cartelle folders = [Distortion, NoFX, Tremolo]\n",
        "#ciclo for x in folders.length analizzate tutte le cartelle (es folders[0] = \"Distortion\" mi muovo in questa cartella e analizzo ogni elemento, folders[1], folders[2]); x parte da 0\n",
        "\n",
        "#consiglio: mettete tutti le analisi in un file json, ve lo salvate e poi analizzate i valori all'interno senza dover riprocessare tutto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6hn6mCIBq-"
      },
      "source": [
        "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
        "for i in range(1, 21):\n",
        "    header += f' mfcc{i}'\n",
        "header += ' label'\n",
        "header = header.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzYgsFKcj6EN"
      },
      "source": [
        "#files path (DA AGGIUSTARE E PROVARE)\n",
        "curr_dir =  \"/content/drive/My Drive/path/to/files/\"\n",
        "%cd \"$curr_dir\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5alKVUOITs2"
      },
      "source": [
        "file = open('data.csv', 'w', newline='')\n",
        "with file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "effects = 'NoFX Tremolo Distortion'.split()\n",
        "for fx in effects:\n",
        "    for filename in os.listdir(f'/content/drive/MyDrive/mfcctest/DatasetTest/{fx}'):\n",
        "        songname = f'/content/drive/MyDrive/mfcctest/DatasetTest/{fx}/{filename}'\n",
        "        y, sr = librosa.load(songname, mono=True, duration=30)\n",
        "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        rmse = librosa.feature.rms(y=y)\n",
        "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
        "        for e in mfcc:\n",
        "            to_append += f' {np.mean(e)}'\n",
        "        to_append += f' {fx}'\n",
        "        file = open('data.csv', 'a', newline='')\n",
        "        with file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(to_append.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6f2m_srIdhu"
      },
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvgD-rQIgny"
      },
      "source": [
        "# Dropping unneccesary columns\n",
        "data = data.drop(['filename'],axis=1)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo-XXg8vXFEf"
      },
      "source": [
        "## **Feature definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKfVdzoucuik"
      },
      "source": [
        "### MFCC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZzgNnKBXEKQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msN-botzgQwA"
      },
      "source": [
        "THC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjrttOBMgPb9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsF6b14pgZwc"
      },
      "source": [
        "Temporal centroid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwS8E3L3gY4c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at_qOksqaXim"
      },
      "source": [
        "Local Maxima number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz8Nnu01aW7L"
      },
      "source": [
        "def find_maxima(signal):\n",
        "\n",
        "  analytic_signal = hilbert(signal)   #hilbert transformation\n",
        "  amp_env = np.abs(analytic_signal)   \n",
        "\n",
        "  # for local maxima - positions\n",
        "  max = len(argrelextrema(amp_env, np.greater))\n",
        "\n",
        "  # for local minima - positions\n",
        "  #min = argrelextrema(amp_env, np.less)\n",
        "\n",
        "return max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1jVuB8sc9Mh"
      },
      "source": [
        "### Compute training feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTHbtRizdyR3"
      },
      "source": [
        "classes = ['blues', 'disco', 'metal']\n",
        "n_mfcc = 13\n",
        "dict_train_features = {'blues': [], 'metal': [], 'disco': []}\n",
        "\n",
        "for c in classes:\n",
        "    train_root = 'Input/{}/training/'.format(c)\n",
        "    class_train_files = [f for f in os.listdir(train_root) if f.endswith('.wav')]\n",
        "    n_train_samples = len(class_train_files)\n",
        "    \n",
        "    train_features = np.zeros((n_train_samples, n_mfcc))\n",
        "    for index, f in enumerate(class_train_files):\n",
        "        audio, fs = librosa.load(os.path.join(train_root, f), sr=None)\n",
        "        mfcc = compute_mfcc(audio, fs, n_mfcc)\n",
        "        train_features[index, :] = np.mean(mfcc, axis=1)\n",
        "        \n",
        "    dict_train_features[c] = train_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo2s6OkCd2Gr"
      },
      "source": [
        "### Compute testing feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTXuXW2Md6hr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZuoMnhpeBWI"
      },
      "source": [
        "# **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydt-EcB4fVmi"
      },
      "source": [
        "class_0 = 'blues'\n",
        "class_1 = 'disco'\n",
        "class_2 = 'metal'\n",
        "\n",
        "X_train_0 = dict_train_features[class_0]\n",
        "X_train_1 = dict_train_features[class_1]\n",
        "X_train_2 = dict_train_features[class_2]\n",
        "\n",
        "y_train_0 = np.zeros((X_train_0.shape[0],))\n",
        "y_train_1 = np.ones((X_train_1.shape[0],))\n",
        "y_train_2 = np.ones((X_train_2.shape[0],))*2\n",
        "\n",
        "#y_train = np.concatenate((y_train_class_0, y_train_class_1, y_train_class_1), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjdZbKuMfYvQ"
      },
      "source": [
        "X_test_0 = dict_test_features[class_0]\n",
        "X_test_1 = dict_test_features[class_1]\n",
        "X_test_2 = dict_test_features[class_2]\n",
        "\n",
        "\n",
        "y_test_0 = np.zeros((X_test_0.shape[0],))\n",
        "y_test_1 = np.ones((X_test_1.shape[0],))\n",
        "y_test_2 = np.ones((X_test_2.shape[0],))*2\n",
        "\n",
        "y_test_mc = np.concatenate((y_test_0, y_test_1, y_test_2), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMRFPlEFe-3A"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVOOUVQ-fECS"
      },
      "source": [
        "feat_max = np.max(np.concatenate((X_train_0, X_train_1, X_train_2), axis=0), axis=0)\n",
        "feat_min = np.min(np.concatenate((X_train_0, X_train_1, X_train_2), axis=0), axis=0)\n",
        "\n",
        "X_train_0_normalized = (X_train_0 - feat_min) / (feat_max - feat_min)\n",
        "X_train_1_normalized = (X_train_1 - feat_min) / (feat_max - feat_min)\n",
        "X_train_2_normalized = (X_train_2 - feat_min) / (feat_max - feat_min)\n",
        "\n",
        "X_test_0_normalized = (X_test_0 - feat_min) / (feat_max - feat_min)\n",
        "X_test_1_normalized = (X_test_1 - feat_min) / (feat_max - feat_min)\n",
        "X_test_2_normalized = (X_test_2 - feat_min) / (feat_max - feat_min)\n",
        "\n",
        "X_test_mc_normalized = np.concatenate((X_test_0_normalized, X_test_1_normalized, X_test_2_normalized), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrICeYmAfpp-"
      },
      "source": [
        "### Models definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ItjH_4vfm9X"
      },
      "source": [
        "SVM_parameters={\n",
        "    'C': 1,\n",
        "    'kernel': 'rbf',\n",
        "}\n",
        "\n",
        "clf_01 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
        "clf_02 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
        "clf_12 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dhb84SZf0xS"
      },
      "source": [
        "### Training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zE_W1Tof4jQ"
      },
      "source": [
        "clf_01.fit(np.concatenate((X_train_0_normalized, X_train_1_normalized), axis=0), \n",
        "           np.concatenate((y_train_0, y_train_1), axis=0))\n",
        "           \n",
        "clf_02.fit(np.concatenate((X_train_0_normalized, X_train_2_normalized), axis=0), \n",
        "           np.concatenate((y_train_0, y_train_2), axis=0))\n",
        "\n",
        "clf_12.fit(np.concatenate((X_train_1_normalized, X_train_2_normalized), axis=0), \n",
        "           np.concatenate((y_train_1, y_train_2), axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}